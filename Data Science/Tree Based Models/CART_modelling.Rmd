---
title: "Tree Based Models"
author: "Aaron Webb"
date: "8 October 2019"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: TRUE
      smooth_scroll: TRUE
---

# Decision Trees
## Why use this method
Decision trees are great methods to use when we have categorical data. Either the 
objective value (dependent variable) has discrete output values, or the independent 
variables are easily sorted into categories.  

There are two different types of decision trees, the CART modelling or the ID3.

1. **Classification and Regression Tree (CART) Modelling**  
CART modelling refers to decision tree algorithms that can be used for 
classification or regression predictive modelling problems.  
For classification the *gini index * method used to provide an indication of how
pure each leaf is.
It favours larger partitions.  
The regression splitting criterion is the anova method. The equation for this is:
$$ SS_T - (SS_L + SS_R) $$
This is the equivalent of choosing the splitto maximise the between groups sum-of-squares.



2. **Iterative Dichotomiser 3 (ID3)**  
ID3 prefers much smaller partitions, it uses entropy and information gain calculations
in order to create the tree.
Entropy measures the importance of information relative to its size. Put simiply,
we are measuring the chaos from a variable. If we imagine a varible with only 2 values, 
and there is an even amount, the entropy value will be 0.5. 
$$ Entropy(x) = -p_ilog_2(p_i) + -p_jlog_2(p_j) $$ 
Information gain is where we derive the original entropy of the population to 
measure the information gain of each variable. 


When using the `rpart` library in R, it will automatically choose whether to choose classification or regression
dependent on the dependent variable.  
Also, the `parms` arguement in `rpart` allows you to choose between the *gini index* 
or the *information gain* method.

## Fundamentals 
The first node in the diagram is called the root node. Each node represents a single input
variable.
The leaf node is where the path of each node ends, which provides us with a class prediction.

To begin, we need to pick the variable that minimises a cost function. This split 
is then used as the root node. We then continue, moving onto the next node, minimising 
the cost function for the new set of data that has been created from the split.

For *regression predictive* modelling problems the cost function that is minimised
to choose the split points (nodes) is the sum squared errors across the training samples.
The cost function that we need to minimise is:

$$ \sum_{i=1}^{n}\left(y - prediction\right)^2 $$
Where **y** is the training sample and the **prediction** is the predicted output for the model, 
the output from one of the leafs. 




# Code
## Packages
The main CART modelling packages is `rpart`. 

```{r install packages}
library(rpart)
```

## Example in R
Using the example data set, `car90` in the `rpart` package, we can create a CART model 
to help us decide and predict the price of a car.
This is by the `str` function too, which provides us with a summary of the variables 
in the data set. 

Need to clean the data of any NAs.
Make sure we take 30% of the data for testing.

```{r createTheData}
data <- rpart::car90
str(data)
```

Next, we create the CART model using the `rpart` function. 


All about the *Complexity Parameter*. That determines how complex your decision tree is.

Interepting the output from `printcp`:
* The *Root Node Error* is the percent of correctly sorted records at the first node split.   
* The *Rel Error* is 1-R^2, the root mean square error. This is the error for predictions of the data that were used to estimate the model
* The *X-error* is the cross validated error.






# Further Reading
## Appendix
1. <http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/>
2. <https://medium.com/machine-learning-guy/an-introduction-to-decision-tree-learning-id3-algorithm-54c74eb2ad55>
3. <https://stackoverflow.com/questions/9666212/how-to-compute-error-rate-from-a-decision-tree>
4. <https://community.alteryx.com/t5/Alteryx-Designer-Knowledge-Base/Understanding-the-Outputs-of-the-Decision-Tree-Tool/ta-p/144773>
5. <https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf>
